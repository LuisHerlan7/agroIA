{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento de CNN para Clasificaci√≥n de Enfermedades en Plantas\n",
        "\n",
        "Este notebook implementa una red neuronal convolucional (CNN) para clasificar enfermedades en hojas de plantas usando el dataset PlantVillage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Definici√≥n de rutas y constantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rutas y constantes\n",
        "DATASET_PATH = Path('../dataset/raw/plant_diseases')\n",
        "MODEL_PATH = Path('../models/modelo_hojas.h5')\n",
        "\n",
        "# Par√°metros de la imagen (seg√∫n RF-02)\n",
        "IMG_SIZE = (224, 224)  # target_size\n",
        "COLOR_MODE = 'rgb'  # color_mode\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10  # Cantidad peque√±a para demostraci√≥n\n",
        "\n",
        "# Divisi√≥n del dataset\n",
        "TRAIN_SPLIT = 0.8  # 80% entrenamiento, 20% validaci√≥n\n",
        "\n",
        "print(f\"üìÅ Dataset: {DATASET_PATH}\")\n",
        "print(f\"üíæ Modelo se guardar√° en: {MODEL_PATH}\")\n",
        "print(f\"üñºÔ∏è  Tama√±o de imagen: {IMG_SIZE}\")\n",
        "print(f\"üé® Modo de color: {COLOR_MODE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generaci√≥n de Data Generators con ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar que el dataset existe\n",
        "if not DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(f\"El dataset no se encuentra en {DATASET_PATH}\")\n",
        "\n",
        "# Data Generator para entrenamiento con aumento de datos\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalizaci√≥n (valores entre 0 y 1)\n",
        "    rotation_range=20,  # Rotaci√≥n aleatoria hasta 20 grados\n",
        "    width_shift_range=0.2,  # Desplazamiento horizontal\n",
        "    height_shift_range=0.2,  # Desplazamiento vertical\n",
        "    zoom_range=0.2,  # Zoom aleatorio\n",
        "    horizontal_flip=True,  # Volteo horizontal\n",
        "    validation_split=TRAIN_SPLIT  # Divisi√≥n entrenamiento/validaci√≥n\n",
        ")\n",
        "\n",
        "# Data Generator para validaci√≥n (solo normalizaci√≥n, sin aumento)\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalizaci√≥n\n",
        "    validation_split=TRAIN_SPLIT\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Data Generators configurados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generadores de datos para entrenamiento y validaci√≥n\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMG_SIZE,  # Redimensionar a 224x224\n",
        "    color_mode=COLOR_MODE,  # RGB\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',  # Para clasificaci√≥n multiclase\n",
        "    subset='training',  # Subconjunto de entrenamiento\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=IMG_SIZE,\n",
        "    color_mode=COLOR_MODE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',  # Subconjunto de validaci√≥n\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Obtener n√∫mero de clases\n",
        "NUM_CLASSES = len(train_generator.class_indices)\n",
        "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(f\"‚úÖ Generadores creados\")\n",
        "print(f\"üìä N√∫mero de clases: {NUM_CLASSES}\")\n",
        "print(f\"üìà Im√°genes de entrenamiento: {train_generator.samples}\")\n",
        "print(f\"üìâ Im√°genes de validaci√≥n: {val_generator.samples}\")\n",
        "print(f\"\\nüìã Primeras 10 clases: {CLASS_NAMES[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Definici√≥n de la Arquitectura CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear modelo Sequential\n",
        "model = Sequential([\n",
        "    # Primer bloque convolucional\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    \n",
        "    # Segundo bloque convolucional\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    \n",
        "    # Tercer bloque convolucional (opcional, para mejorar rendimiento)\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    \n",
        "    # Aplanar las caracter√≠sticas\n",
        "    Flatten(),\n",
        "    \n",
        "    # Primera capa densa\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Regularizaci√≥n para evitar overfitting\n",
        "    \n",
        "    # Segunda capa densa\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    # Capa de salida con Softmax para clasificaci√≥n multiclase\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Modelo CNN creado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizaci√≥n del Resumen del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Compilaci√≥n del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compilar el modelo\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),  # Optimizador adam\n",
        "    loss='categorical_crossentropy',  # Funci√≥n de p√©rdida para clasificaci√≥n multiclase\n",
        "    metrics=['accuracy']  # M√©trica de precisi√≥n\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo compilado\")\n",
        "print(f\"   Optimizador: Adam\")\n",
        "print(f\"   Funci√≥n de p√©rdida: categorical_crossentropy\")\n",
        "print(f\"   M√©trica: accuracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Entrenamiento del Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular steps por √©poca\n",
        "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
        "validation_steps = val_generator.samples // BATCH_SIZE\n",
        "\n",
        "print(f\"üîÑ Iniciando entrenamiento...\")\n",
        "print(f\"   √âpocas: {EPOCHS}\")\n",
        "print(f\"   Steps por √©poca: {steps_per_epoch}\")\n",
        "print(f\"   Validation steps: {validation_steps}\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Entrenamiento completado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizaci√≥n de Resultados del Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficar accuracy y loss\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history.history['loss'], label='Train Loss')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mostrar m√©tricas finales\n",
        "print(f\"\\nüìä M√©tricas finales:\")\n",
        "print(f\"   Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"   Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"   Train Loss: {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"   Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Asegurar que el directorio models existe\n",
        "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(MODEL_PATH)\n",
        "\n",
        "print(f\"‚úÖ Modelo guardado en: {MODEL_PATH}\")\n",
        "print(f\"üì¶ Tama√±o del archivo: {MODEL_PATH.stat().st_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "# Guardar tambi√©n los nombres de las clases para uso futuro\n",
        "import json\n",
        "class_indices_path = MODEL_PATH.parent / 'class_indices.json'\n",
        "with open(class_indices_path, 'w') as f:\n",
        "    json.dump(train_generator.class_indices, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ √çndices de clases guardados en: {class_indices_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Resumen y Pr√≥ximos Pasos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Entrenamiento completado exitosamente\")\n",
        "print(\"\\nüìù Pr√≥ximos pasos:\")\n",
        "print(\"  1. El modelo est√° guardado en models/modelo_hojas.h5\")\n",
        "print(\"  2. Los √≠ndices de clases est√°n en models/class_indices.json\")\n",
        "print(\"  3. Integrar el modelo en el backend (app/utils.py y backend/main.py)\")\n",
        "print(\"  4. Probar el endpoint /predict desde el frontend\")\n",
        "print(\"\\nüí° Nota: Si el accuracy es bajo, considera:\")\n",
        "print(\"  - Aumentar el n√∫mero de √©pocas\")\n",
        "print(\"  - Ajustar el learning rate\")\n",
        "print(\"  - Agregar m√°s capas convolucionales\")\n",
        "print(\"  - Usar transfer learning (ResNet, VGG, etc.)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
